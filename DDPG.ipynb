{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weihangoh/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/weihangoh/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "/Users/weihangoh/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "<string>:37: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "<string>:38: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import random\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib import animation\n",
    "# from JSAnimation.IPython_display import display_animation\n",
    "# import gym\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "from collections import namedtuple\n",
    "import statistics\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import import_ipynb\n",
    "import Generate_Data\n",
    "import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Classes\n",
    "- Actor: A neural network model that represents the policy in the DDPG algorithm, mapping states to continuous actions.\n",
    "\n",
    "- Critic: A neural network model that estimates the Q-values, evaluating the quality of state-action pairs in the DDPG framework.\n",
    "\n",
    "- ReplayBuffer: A data structure that stores past experiences (state, action, reward, next_state, done) to facilitate experience replay during training.\n",
    "\n",
    "- OUNoise: Implements the Ornstein-Uhlenbeck process to generate temporally correlated noise, promoting exploration in continuous action spaces.\n",
    "\n",
    "- DDPGAgent: Encapsulates the entire DDPG agent, integrating the Actor and Critic networks, Replay Buffer, and OUNoise, and providing methods for action selection, experience memorization, and network updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action = self.max_action * torch.tanh(self.fc3(x))\n",
    "        return action\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Q Network\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.relu(self.fc1(torch.cat([state, action], dim=1)))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = 1\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "    \n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state.squeeze()  # Return as scalar if action_dim == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action, \n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.ou_noise = OUNoise(action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def get_action(self, state, step=0, episode=0):\n",
    "        return self.decide_action(state, step, episode)\n",
    "\n",
    "    def decide_action(self, state, step, episode, add_noise=True):\n",
    "        \"\"\"\n",
    "        Selects an action based on the given state, optionally adding exploration noise.\n",
    "        \"\"\"\n",
    "        # Convert state to tensor and get action from the actor\n",
    "        state = torch.FloatTensor(state).to(device).unsqueeze(0)  # Ensure 2D input for batch processing\n",
    "        action = self.actor(state).cpu().data.numpy().flatten().item()  # Convert to scalar\n",
    "\n",
    "        # Add noise for exploration if desired\n",
    "        if add_noise:\n",
    "            noise = self.ou_noise.noise() * self.max_action * 0.2  # Adjust noise scale\n",
    "            # Ensure noise is scalar\n",
    "            if isinstance(noise, np.ndarray):\n",
    "                noise = noise.item() if noise.size == 1 else noise[0]\n",
    "            elif isinstance(noise, float) or isinstance(noise, int):\n",
    "                noise = noise\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected noise type\")\n",
    "            \n",
    "            action += noise\n",
    "            action = np.clip(action, -self.max_action, self.max_action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Stores experience in the replay buffer.\n",
    "        \"\"\"\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_agent(self, batch_size=64, gamma=0.99, tau=0.005):\n",
    "        \"\"\"\n",
    "        Performs a training step for the agent.\n",
    "        \"\"\"\n",
    "        if self.replay_buffer.size() < batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action = torch.FloatTensor(action).to(device).unsqueeze(1)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        done = torch.FloatTensor(done).unsqueeze(1).to(device)\n",
    "\n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor_target(next_state)\n",
    "            target_Q = self.critic_target(next_state, next_action)\n",
    "            target_Q = reward + (1 - done) * gamma * target_Q\n",
    "\n",
    "        current_Q = self.critic(state, action)\n",
    "\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor update\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Target networks update\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, num_episodes=1000, num_steps=30, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Trains the agent in the given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The trading environment (Environment).\n",
    "    - agent: The DDPG agent.\n",
    "    - num_episodes: The number of training episodes.\n",
    "    - num_steps: The number of steps per episode.\n",
    "    - gamma: Discount factor for future rewards.\n",
    "\n",
    "    Returns:\n",
    "    - pl_history: A list containing the P&L history for each episode.\n",
    "    - reward_history: A list containing the cumulative reward history for each episode.\n",
    "    \"\"\"\n",
    "    \n",
    "    pl_history = []  # Profit and loss history for each episode\n",
    "    reward_history = []  # Total reward history for each episode\n",
    "    std_pl_history = []  # Standard deviation of P&L for each episode ; added this \n",
    "\n",
    "    last_30_pl = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()  # Reset the environment and get the initial state\n",
    "        total_reward = 0  # Track total reward for the episode\n",
    "        pl_episode = []  # Track portfolio value (P&L) for the episode\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Agent takes an action based on the current state\n",
    "            action = agent.get_action(state, step=step, episode=episode)\n",
    "            \n",
    "            # Environment returns the next state, reward, and whether the episode is done\n",
    "            next_state, reward, done, pv = env.step(action)\n",
    "            \n",
    "            # Accumulate total reward, discounted by gamma (for future rewards)\n",
    "            total_reward += reward * (gamma ** step)\n",
    "            \n",
    "            # Store the portfolio value (P&L) for the step\n",
    "            pl_episode.append(pv)\n",
    "            \n",
    "            # Memorize the experience with the done flag\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update the agent with experiences from the replay buffer\n",
    "            agent.update_agent()\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            # If the episode is done, break out of the loop\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # At the end of the episode, calculate the total P&L and reward\n",
    "        pl_total = sum(pl_episode)\n",
    "        pl_history.append(pl_total)\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        last_30_pl.append(pl_total)\n",
    "        if len(last_30_pl) > 30:\n",
    "            last_30_pl.pop(0)\n",
    "\n",
    "        if episode >= 30:\n",
    "            std = np.std(last_30_pl)\n",
    "            std_pl_history.append(std)\n",
    "        else:\n",
    "            std_pl_history.append(np.nan)\n",
    "\n",
    "        # if episode % 30 == 0 and episode != 0:\n",
    "        #     std_pl_history.append(np.std(pl_episode[-30:]))  # Compute std dev of P&L for the episode # added this\n",
    "        \n",
    "        # Print progress every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes}, Total Reward: {total_reward}, P&L: {pl_total}, Std Dev P&L: {std_pl_history[-1]}\") # added std dev part\n",
    "    \n",
    "    return pl_history, reward_history, std_pl_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Environment' object has no attribute 'calculate_gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m ddpg_agent \u001b[38;5;241m=\u001b[39m DDPGAgent(num_states, num_actions, max_action)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train the agen\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m pl_history, reward_history, std_pl \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddpg_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Plot P&L history\u001b[39;00m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, num_episodes, num_steps, gamma)\u001b[0m\n\u001b[1;32m     21\u001b[0m last_30_pl \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 24\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reset the environment and get the initial state\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Track total reward for the episode\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     pl_episode \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Track portfolio value (P&L) for the episode\u001b[39;00m\n",
      "File \u001b[0;32m<string>:37\u001b[0m, in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m<string>:107\u001b[0m, in \u001b[0;36m_get_state\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Environment' object has no attribute 'calculate_gamma'"
     ]
    }
   ],
   "source": [
    "##Getting train datasets\n",
    "features_train = Generate_Data.features_train\n",
    "df_stock_train = Generate_Data.df_stock_train\n",
    "call_options_train = Generate_Data.call_options_train\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up your environment with real AAPL stock and option data\n",
    "env_train = Environment.Environment(\n",
    "    stock_data=df_stock_train,\n",
    "    option_data=call_options_train,\n",
    "    features_data=features_train,\n",
    "    T=30,\n",
    "    n_steps=30,\n",
    "    num_sold_opt=100,\n",
    "    kappa=0.1,\n",
    "    alpha=0.001\n",
    ")\n",
    "num_states = env_train.num_states\n",
    "num_actions = env_train.num_actions\n",
    "max_action = 10\n",
    "\n",
    "ddpg_agent = DDPGAgent(num_states, num_actions, max_action)\n",
    "\n",
    "# Train the agen\n",
    "pl_history, reward_history, std_pl = train_agent(env_train, ddpg_agent, num_episodes=1000, num_steps=30)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot P&L history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(pl_history)\n",
    "plt.title(\"P&L History\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"P&L\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pl_history, kde=False, bins=30, label=\"Profit and Loss\", alpha=0.6)\n",
    "\n",
    "plt.axvline(0, color='k', linestyle='--')  # Mark zero for reference\n",
    "plt.xlabel(\"Profit and Loss\")\n",
    "plt.ylabel(\"Frequency / Density\")\n",
    "plt.title(\"Histogram with KDE of Profit and Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = np.array(list(range(1, len(reward_history) + 1)))\n",
    "\n",
    "# Create a figure for dual-axis plotting\n",
    "fig = plt.figure(figsize=[6, 4])\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Plot Sum of Discounted Reward\n",
    "ax1.plot(episodes, reward_history, color='coral', label='Sum Reward')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel(\"Sum of Discounted Reward\")\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "\n",
    "# Create a secondary axis for Std Dev PL\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(episodes, std_pl, color='forestgreen', label='Std Dev PL')\n",
    "ax2.set_ylabel(\"Std. Dev. of PL\")\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Combine legends for both axes and set the title\n",
    "ax2.legend(h1 + h2, l1 + l2, loc='center right')\n",
    "ax1.set_title('Case of No Costs: Delta')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the entire agent\n",
    "with open('models/ddpg_agent_full.pkl', 'wb') as f:\n",
    "    pickle.dump(ddpg_agent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Summary\n",
    "Train DDPG model, save as ddpg_agent_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
